{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_Sentence_Classification_Colton_Alexis_dataset.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rrupam/BERT_sentence_classification/blob/master/BERT_Sentence_Classification_Colton_Alexis_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8Nihl0cmh_e",
        "colab_type": "text"
      },
      "source": [
        "Following  [this]( https://towardsdatascience.com/bert-classifier-just-another-pytorch-model-881b3cf05784) blog for sentence classification using a pretrained BERT model, with multiclass, instead of binary. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3vg4xdi-h3h",
        "colab_type": "code",
        "outputId": "3be4341f-a390-468a-ed16-dbd3bbedc7e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_oXjrFC_-BG",
        "colab_type": "code",
        "outputId": "f5c0cb7c-aff4-40aa-c2c0-b583b0490500",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "% cd drive/My Drive/Colab Notebooks/PTSD/Sentiment_Analysis"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/PTSD/Sentiment_Analysis\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkPd7Kk6Ainl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "from random import randrange\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxBm6BTgATeh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv('PTSD_data_Colton_Alexis.csv', nrows = 865)# limiting the number of rows to only the text that has already been annotated by Alexis\n",
        "#,delimiter='\\t',encoding='utf-8', nrows=10000)#, encoding =\"ISO-8859-1\" , names=DATASET_COLUMNS, nrows=10000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvvhUlPDAVQX",
        "colab_type": "code",
        "outputId": "f00a4241-d30b-4efa-a3f7-864d79a2fa5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        }
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Unnamed: 1</th>\n",
              "      <th>Unnamed: 2</th>\n",
              "      <th>Unnamed: 3</th>\n",
              "      <th>Exposure</th>\n",
              "      <th>Intrusive Memories</th>\n",
              "      <th>Distressing dreams</th>\n",
              "      <th>Dissociative reactions</th>\n",
              "      <th>Psychological distress at exposure to related cues</th>\n",
              "      <th>Physiological reactions at exposure to related cues</th>\n",
              "      <th>Avoidance to internals</th>\n",
              "      <th>Avoidance to externals</th>\n",
              "      <th>Dissociative amnesia</th>\n",
              "      <th>Negative Beliefs</th>\n",
              "      <th>Distorted Blaming</th>\n",
              "      <th>Negative emotional state</th>\n",
              "      <th>Low significant activities</th>\n",
              "      <th>Detachment</th>\n",
              "      <th>Inability to experience positive emotions</th>\n",
              "      <th>Irritability Anger</th>\n",
              "      <th>Self-destructive behavior</th>\n",
              "      <th>Hypervigilance</th>\n",
              "      <th>Exaggerated startle response</th>\n",
              "      <th>Problems with concentration</th>\n",
              "      <th>Sleep disturbance</th>\n",
              "      <th>Over one month</th>\n",
              "      <th>Distress / interfere</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>A: Exposure to actual or threatened death, ser...</td>\n",
              "      <td>B: Presence of intrusion symptoms associated w...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Criterion C: Persistent avoidance of stimuli a...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Criterion D: Negative alterations in cognition...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Criterion E: Marked alterations in arousal and...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>F: Duration of the disturbance (Criteria B, C,...</td>\n",
              "      <td>G: The symptoms bring about considerable distr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>(B1): Recurrent, involuntary, and intrusive di...</td>\n",
              "      <td>(B2): Recurrent distressing dreams in which th...</td>\n",
              "      <td>(B3): Dissociative reactions (e.g., flashbacks...</td>\n",
              "      <td>(B4): Intense or prolonged psychological distr...</td>\n",
              "      <td>(B5): Marked physiological reactions to intern...</td>\n",
              "      <td>(C1): Avoidance of or efforts to avoid distres...</td>\n",
              "      <td>(C2): Avoidance of or efforts to avoid externa...</td>\n",
              "      <td>(D1): Inability to remember an important aspec...</td>\n",
              "      <td>(D2): Persistent and exaggerated negative beli...</td>\n",
              "      <td>(D3): Persistent, distorted cognitions about t...</td>\n",
              "      <td>(D4): Persistent negative emotional state (e.g...</td>\n",
              "      <td>(D5): Markedly diminished interest or particip...</td>\n",
              "      <td>(D6): Feelings of detachment or estrangement f...</td>\n",
              "      <td>(D7): Persistent inability to experience posit...</td>\n",
              "      <td>(E1): Irritable behavior and angry outbursts (...</td>\n",
              "      <td>(E2): Reckless or self-destructive behavior.</td>\n",
              "      <td>(E3): Hypervigilance.</td>\n",
              "      <td>(E4): Exaggerated startle response.</td>\n",
              "      <td>(E5): Problems with concentration.</td>\n",
              "      <td>(E6): Sleep disturbance (e.g., difficulty fall...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>transcript_id</td>\n",
              "      <td>character</td>\n",
              "      <td>text</td>\n",
              "      <td>Keywords / Significant sentences</td>\n",
              "      <td>A1</td>\n",
              "      <td>B1</td>\n",
              "      <td>B2</td>\n",
              "      <td>B3</td>\n",
              "      <td>B4</td>\n",
              "      <td>B5</td>\n",
              "      <td>C1</td>\n",
              "      <td>C2</td>\n",
              "      <td>D1</td>\n",
              "      <td>D2</td>\n",
              "      <td>D3</td>\n",
              "      <td>D4</td>\n",
              "      <td>D5</td>\n",
              "      <td>D6</td>\n",
              "      <td>D7</td>\n",
              "      <td>E1</td>\n",
              "      <td>E2</td>\n",
              "      <td>E3</td>\n",
              "      <td>E4</td>\n",
              "      <td>E5</td>\n",
              "      <td>E6</td>\n",
              "      <td>F1</td>\n",
              "      <td>G1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>PTSD_file_1</td>\n",
              "      <td>CLIENT</td>\n",
              "      <td>Remind me never to go to a work meeting with a...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>PTSD_file_1</td>\n",
              "      <td>THERAPIST</td>\n",
              "      <td>Those darn women.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Unnamed: 0  ...                               Distress / interfere\n",
              "0            NaN  ...  G: The symptoms bring about considerable distr...\n",
              "1            NaN  ...                                                NaN\n",
              "2  transcript_id  ...                                                 G1\n",
              "3    PTSD_file_1  ...                                                  0\n",
              "4    PTSD_file_1  ...                                                  0\n",
              "\n",
              "[5 rows x 27 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CimQliPCE76G",
        "colab_type": "code",
        "outputId": "6592a764-8683-4bec-cfa3-8af34a7b88e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 593
        }
      },
      "source": [
        "data.info()"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 865 entries, 0 to 864\n",
            "Data columns (total 27 columns):\n",
            "Unnamed: 0                                             863 non-null object\n",
            "Unnamed: 1                                             863 non-null object\n",
            "Unnamed: 2                                             863 non-null object\n",
            "Unnamed: 3                                             42 non-null object\n",
            "Exposure                                               863 non-null object\n",
            "Intrusive Memories                                     864 non-null object\n",
            "Distressing dreams                                     863 non-null object\n",
            "Dissociative reactions                                 863 non-null object\n",
            "Psychological distress at exposure to related cues     863 non-null object\n",
            "Physiological reactions at exposure to related cues    863 non-null object\n",
            "Avoidance to internals                                 864 non-null object\n",
            "Avoidance to externals                                 863 non-null object\n",
            "Dissociative amnesia                                   864 non-null object\n",
            "Negative Beliefs                                       863 non-null object\n",
            "Distorted Blaming                                      863 non-null object\n",
            "Negative emotional state                               863 non-null object\n",
            "Low significant activities                             863 non-null object\n",
            "Detachment                                             863 non-null object\n",
            "Inability to experience positive emotions              863 non-null object\n",
            "Irritability Anger                                     864 non-null object\n",
            "Self-destructive behavior                              863 non-null object\n",
            "Hypervigilance                                         863 non-null object\n",
            "Exaggerated startle response                           863 non-null object\n",
            "Problems with concentration                            863 non-null object\n",
            "Sleep disturbance                                      863 non-null object\n",
            "Over one month                                         863 non-null object\n",
            "Distress / interfere                                   863 non-null object\n",
            "dtypes: object(27)\n",
            "memory usage: 182.5+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIadHuZnAj1k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = data.replace('0',0)\n",
        "data = data.replace('1',1)\n",
        "data = data.replace('0.5',0.5)\n",
        "data = data.fillna(0)\n",
        "data = data.drop([0,1,2])\n",
        "data.columns=['Transcript_id','Character','Text','Keywords','A1','B1','B2','B3','B4','B5','C1','C2','D1','D2','D3','D4','D5','D6','D7','E1','E2','E3','E4','E5','E6','F1','G1']\n",
        "#help(data.drop)\n",
        "data = data[data['Character']=='CLIENT'] #Extracting only the client transcripts for training"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVa5THniW1kq",
        "colab_type": "text"
      },
      "source": [
        "We convert the datatype of A1,A2.. etc from 'object' to 'numeric' in order to perform some data anlysis. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHihqFPrBaff",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classes = ['A1','B1','B2','B3','B4','B5','C1','C2','D1','D2','D3','D4','D5','D6','D7','E1','E2','E3','E4','E5','E6','F1','G1']\n",
        "\n",
        "for c in classes:\n",
        "  data[c] = pd.to_numeric(data[c],errors='coerce')\n",
        "  \n",
        "data['Text']=data['Text'].apply(str)  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Xuh0k_oV1_i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "3d0e677a-cb80-40de-fdd1-f343a521ed0a"
      },
      "source": [
        "plt.hist(data['A1'], bins=3)\n",
        "plt.hist(data['B1'],bins = 3)\n",
        "plt.show()"
      ],
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAADqJJREFUeJzt3X2MpWV9xvHvJSvaRmV5mSLd3XYw\nrmmpTZFuCMa0WtEGwbgkVYupdTWbblTaaGzTbusftS9/gI1SjcaGCHE1vkBtEzaKaXGFkBpBh4Io\nEGWkGHZFdlWgNUYr9dc/zr12djuzc2bnvHHz/SQncz/3c88515xZrnnmOWceUlVIkvr1pGkHkCSN\nl0UvSZ2z6CWpcxa9JHXOopekzln0ktQ5i16SOmfRS1LnLHpJ6tyGaQcAOO2002p+fn7aMSTpceW2\n2277TlXNrbZuJop+fn6ehYWFaceQpMeVJN8cZp2nbiSpcxa9JHXOopekzln0ktQ5i16SOmfRS1Ln\nLHpJ6pxFL0mds+glqXMz8ZexGqF3nDTtBLPlHY9OO4E0dR7RS1LnLHpJ6pxFL0md8xy9uja/+9PT\njsD9l1007Qh6gvOIXpI6Z9FLUucseknqnEUvSZ2z6CWpcxa9JHXOopekzln0ktS5oYs+yQlJbk/y\nqbZ9ZpJbkywmuSbJiW3+KW17se2fH090SdIw1nJE/xbgniXblwNXVNWzgYeBnW1+J/Bwm7+irZMk\nTclQRZ9kM3AR8MG2HeDFwCfbkj3AxW28vW3T9p/f1kuSpmDYI/q/B/4U+EnbPhV4pKoea9v7gU1t\nvAl4AKDtf7StlyRNwapFn+TlwMGqum2UD5xkV5KFJAuHDh0a5V1LkpYY5oj+BcArktwPfILBKZv3\nABuTHL765WbgQBsfALYAtP0nAd89+k6r6sqq2lZV2+bm5tb1RUiSVrZq0VfVn1fV5qqaBy4BPldV\nvwfcCLyyLdsBXNfGe9s2bf/nqqpGmlqSNLT1vI/+z4C3JVlkcA7+qjZ/FXBqm38bsHt9ESVJ67Gm\n//FIVd0E3NTG9wHnLrPmh8CrRpBNkjQC/mWsJHXOopekzln0ktQ5i16SOmfRS1LnLHpJ6pxFL0md\ns+glqXMWvSR1zqKXpM5Z9JLUOYtekjpn0UtS5yx6SeqcRS9JnbPoJalzFr0kdc6il6TOWfSS1DmL\nXpI6Z9FLUucseknqnEUvSZ2z6CWpcxa9JHXOopekzln0ktQ5i16SOmfRS1LnLHpJ6pxFL0mds+gl\nqXMWvSR1zqKXpM5Z9JLUOYtekjpn0UtS5yx6SeqcRS9JnVu16JM8NckXk3w5yV1J/qrNn5nk1iSL\nSa5JcmKbf0rbXmz758f7JUiSjmWYI/ofAS+uql8DzgYuSHIecDlwRVU9G3gY2NnW7wQebvNXtHWS\npClZtehr4Ptt88ntVsCLgU+2+T3AxW28vW3T9p+fJCNLLElak6HO0Sc5IckdwEHgBuAbwCNV9Vhb\nsh/Y1MabgAcA2v5HgVOXuc9dSRaSLBw6dGh9X4UkaUVDFX1V/U9VnQ1sBs4Ffmm9D1xVV1bVtqra\nNjc3t967kyStYE3vuqmqR4AbgecDG5NsaLs2Awfa+ACwBaDtPwn47kjSSpLWbJh33cwl2djGPwO8\nFLiHQeG/si3bAVzXxnvbNm3/56qqRhlakjS8Dasv4QxgT5ITGPxguLaqPpXkbuATSf4WuB24qq2/\nCvhIkkXge8AlY8gtSRrSqkVfVXcCz1tm/j4G5+uPnv8h8KqRpJMkrZt/GStJnbPoJalzFr0kdc6i\nl6TOWfSS1DmLXpI6Z9FLUucseknqnEUvSZ2z6CWpcxa9JHXOopekzln0ktQ5i16SOmfRS1LnLHpJ\n6pxFL0mds+glqXMWvSR1zqKXpM5Z9JLUOYtekjpn0UtS5yx6SeqcRS9JnbPoJalzFr0kdc6il6TO\nWfSS1DmLXpI6Z9FLUucseknqnEUvSZ2z6CWpcxa9JHXOopekzln0ktQ5i16SOrdq0SfZkuTGJHcn\nuSvJW9r8KUluSHJv+3hym0+S9yZZTHJnknPG/UVIklY2zBH9Y8AfV9VZwHnApUnOAnYD+6pqK7Cv\nbQO8DNjabruAD4w8tSRpaKsWfVU9WFX/3sb/BdwDbAK2A3vasj3AxW28HfhwDdwCbExyxsiTS5KG\nsqZz9EnmgecBtwKnV9WDbde3gdPbeBPwwJJP29/mJElTMHTRJ3ka8E/AW6vqP5fuq6oCai0PnGRX\nkoUkC4cOHVrLp0qS1mCook/yZAYl/9Gq+uc2/dDhUzLt48E2fwDYsuTTN7e5I1TVlVW1raq2zc3N\nHW9+SdIqhnnXTYCrgHuq6t1Ldu0FdrTxDuC6JfOva+++OQ94dMkpHknShG0YYs0LgN8HvpLkjjb3\nF8BlwLVJdgLfBF7d9l0PXAgsAj8A3jDSxJKkNVm16Kvq34CssPv8ZdYXcOk6c0mSRsS/jJWkzln0\nktQ5i16SOmfRS1LnLHpJ6pxFL0mds+glqXMWvSR1zqKXpM5Z9JLUOYtekjpn0UtS5yx6SeqcRS9J\nnbPoJalzFr0kdc6il6TOWfSS1DmLXpI6Z9FLUucseknqnEUvSZ2z6CWpcxa9JHXOopekzln0ktQ5\ni16SOmfRS1LnLHpJ6pxFL0mds+glqXMWvSR1zqKXpM5Z9JLUOYtekjpn0UtS5yx6SeqcRS9JnbPo\nJalzqxZ9kquTHEzy1SVzpyS5Icm97ePJbT5J3ptkMcmdSc4ZZ3hJ0uqGOaL/EHDBUXO7gX1VtRXY\n17YBXgZsbbddwAdGE1OSdLxWLfqquhn43lHT24E9bbwHuHjJ/Idr4BZgY5IzRhVWkrR2x3uO/vSq\nerCNvw2c3sabgAeWrNvf5v6fJLuSLCRZOHTo0HHGkCStZt0vxlZVAXUcn3dlVW2rqm1zc3PrjSFJ\nWsHxFv1Dh0/JtI8H2/wBYMuSdZvbnCRpSo636PcCO9p4B3DdkvnXtXffnAc8uuQUjyRpCjastiDJ\nx4EXAacl2Q/8JXAZcG2SncA3gVe35dcDFwKLwA+AN4whsyRpDVYt+qp6zQq7zl9mbQGXrjeUJGl0\n/MtYSeqcRS9JnbPoJalzFr0kdc6il6TOWfSS1DmLXpI6Z9FLUucseknqnEUvSZ2z6CWpcxa9JHXO\nopekzln0ktQ5i16SOmfRS1LnLHpJ6pxFL0mds+glqXMWvSR1zqKXpM5Z9JLUOYtekjpn0UtS5yx6\nSeqcRS9JnbPoJalzFr0kdc6il6TOWfSS1DmLXpI6Z9FLUucseknqnEUvSZ2z6CWpcxa9JHXOopek\nzln0ktS5sRR9kguSfC3JYpLd43gMSdJwNoz6DpOcALwfeCmwH/hSkr1VdfeoH0t6PJjf/elpR5gp\n91920bQjPOGMvOiBc4HFqroPIMkngO2ARS/JH3xHmcQPvnGcutkEPLBke3+bkyRNwTiO6IeSZBew\nq21+P8nXppVlGacB35l2iGOY9XwwMxlfvtKOGcl3TLOecdbzweMgYy5fV8ZfHGbROIr+ALBlyfbm\nNneEqroSuHIMj79uSRaqatu0c6xk1vPB7Gec9Xww+xlnPR+Y8bBxnLr5ErA1yZlJTgQuAfaO4XEk\nSUMY+RF9VT2W5A+BfwFOAK6uqrtG/TiSpOGM5Rx9VV0PXD+O+56QmTyltMSs54PZzzjr+WD2M856\nPjAjAKmqcT+GJGmKvASCJHXOogeSnJLkhiT3to8nH2PtM5LsT/K+WcqX5OwkX0hyV5I7k/zuBHId\n81IXSZ6S5Jq2/9Yk8+POdBwZ35bk7vac7Usy1NvVJplxybrfSVJJJvoukmHyJXl1ex7vSvKxSeYb\nJmOSX0hyY5Lb2/f6wgnnuzrJwSRfXWF/kry35b8zyTkjDVBVT/gb8E5gdxvvBi4/xtr3AB8D3jdL\n+YDnAFvb+OeBB4GNY8x0AvAN4FnAicCXgbOOWvNm4B/a+BLgmgl/X4fJ+FvAz7bxm2YxY1v3dOBm\n4BZg2yzlA7YCtwMnt+2fm7XnkMF58De18VnA/RPO+JvAOcBXV9h/IfAZIMB5wK2jfHyP6Ae2A3va\neA9w8XKLkvw6cDrwrxPKddiq+arq61V1bxt/CzgIzI0x008vdVFV/w0cvtTFUktzfxI4P0nGmGnN\nGavqxqr6Qdu8hcHffUzSMM8jwN8AlwM/nGQ4hsv3B8D7q+phgKo6OIMZC3hGG58EfGuC+aiqm4Hv\nHWPJduDDNXALsDHJGaN6fIt+4PSqerCNv82gzI+Q5EnAu4A/mWSwZtV8SyU5l8GRzTfGmGmYS138\ndE1VPQY8Cpw6xkxHW+vlOHYyOKqapFUztl/jt1TVNC4SM8xz+BzgOUk+n+SWJBdMLN3AMBnfAbw2\nyX4G7wj8o8lEG9pYLx0ztUsgTFqSzwLPXGbX25duVFUlWe6tSG8Grq+q/eM4KB1BvsP3cwbwEWBH\nVf1ktCn7leS1wDbghdPOslQ7wHg38PopRzmWDQxO37yIwW9ENyf51ap6ZKqpjvQa4ENV9a4kzwc+\nkuS5T5T/Rp4wRV9VL1lpX5KHkpxRVQ+2olzuV8/nA7+R5M3A04ATk3y/qkZyvf0R5CPJM4BPA29v\nv/6N0zCXuji8Zn+SDQx+Zf7umHMt9/iHLXs5jiQvYfAD9YVV9aMJZTtstYxPB54L3NQOMJ4J7E3y\niqpamIF8MDj6vLWqfgz8R5KvMyj+L00gHwyXcSdwAUBVfSHJUxlcB2fSp5lWMtS/1eM2yRckZvUG\n/B1Hvtj5zlXWv57Jvhi7aj4Gp2r2AW+dUKYNwH3AmfzfC2C/ctSaSznyxdhrJ/x9HSbj8xic4to6\npX97q2Y8av1NTPbF2GGewwuAPW18GoNTEKfOWMbPAK9v419mcI4+E/5ez7Pyi7EXceSLsV8c6WNP\n8gud1RuD88b7gHuBzwKntPltwAeXWT/pol81H/Ba4MfAHUtuZ48514XA11tRvr3N/TXwijZ+KvCP\nwCLwReBZU/jerpbxs8BDS56zvbOW8ai1Ey36IZ/DMDi9dDfwFeCSWXsOGbzT5vPth8AdwG9PON/H\nGbwT7scMfgPaCbwReOOS5/D9Lf9XRv099i9jJalzvutGkjpn0UtS5yx6SeqcRS9JnbPoJalzFr0k\ndc6il6TOWfSS1Ln/BfFVPwvF4xjBAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRiflTqeBs25",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "package_dir = \"../input/ppbert/pytorch-pretrained-bert/pytorch-pretrained-BERT\"\n",
        "sys.path.append(package_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9i4V2DKqnAds",
        "colab_type": "text"
      },
      "source": [
        "# **Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7b2Js9GxFChQ",
        "colab_type": "code",
        "outputId": "75979926-929d-447c-a426-40efda0196f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "!pip install pytorch-pretrained-bert"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-pretrained-bert in /usr/local/lib/python3.6/dist-packages (0.6.2)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.9.189)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2019.6.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.21.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.16.4)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.28.1)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.189 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (1.12.189)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.2.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2019.6.16)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.189->boto3->pytorch-pretrained-bert) (0.14)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.189->boto3->pytorch-pretrained-bert) (2.5.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.189->boto3->pytorch-pretrained-bert) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_qHFNX5FEUA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "import torch.utils.data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import warnings\n",
        "#import pytorch_pretrained_bert\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertForSequenceClassification, BertAdam\n",
        "from pytorch_pretrained_bert import BertConfig\n",
        "from pytorch_pretrained_bert.modeling import BertModel, BertForMaskedLM\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels = 5)\n",
        "\n",
        "warnings.filterwarnings(action='once')\n",
        "device = torch.device('cuda')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hE3doMeMFITS",
        "colab_type": "code",
        "outputId": "dacca889-549e-4369-ebec-098e252d3cb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 1091356.64B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXDIJUHYH7pW",
        "colab_type": "code",
        "outputId": "32401b17-485b-43ed-86a9-8b6a02de5987",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "#sentiment_columns = ['Negative','Somewhat Negative','Neutral','Somewhat Positive', 'Positive']\n",
        "classes=['A1','B1','B2','B3','B4','B5','C1','C2','D1','D2','D3','D4','D5','D6','D7','E1','E2','E3','E4','E5','E6','F1','G1']\n",
        "X = data['Text']\n",
        "y = data[classes]\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=42)"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
            "  return f(*args, **kwds)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlIJ9PAxIe3J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.values.tolist()\n",
        "X_test = X_test.values.tolist()\n",
        "\n",
        "y_train = y_train.values.tolist()\n",
        "y_test = y_test.values.tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZ6t5TOCnKCc",
        "colab_type": "text"
      },
      "source": [
        "Now we create a class to preprocess data that involves tokenizing, truncating and padding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rE8xKgazJ29_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_seq_length = 256\n",
        "class text_dataset(Dataset):\n",
        "    def __init__(self,x_y_list, transform=None):\n",
        "        \n",
        "        self.x_y_list = x_y_list\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __getitem__(self,index):\n",
        "        \n",
        "        tokenized_review = tokenizer.tokenize(self.x_y_list[0][index])\n",
        "        \n",
        "        if len(tokenized_review) > max_seq_length:\n",
        "            tokenized_review = tokenized_review[:max_seq_length]\n",
        "            \n",
        "        ids_review  = tokenizer.convert_tokens_to_ids(tokenized_review)\n",
        "\n",
        "        padding = [0] * (max_seq_length - len(ids_review))\n",
        "        \n",
        "        ids_review += padding\n",
        "        \n",
        "        assert len(ids_review) == max_seq_length\n",
        "        \n",
        "        #print(\"Hello\")\n",
        "        ids_review = torch.tensor(ids_review).long()\n",
        "        \n",
        "        sentiment = self.x_y_list[1][index] # color        \n",
        "        list_of_labels = [torch.from_numpy(np.array(sentiment)).long()]\n",
        "        \n",
        "        \n",
        "        return ids_review, list_of_labels[0]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.x_y_list[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQrQTCRrnZXt",
        "colab_type": "text"
      },
      "source": [
        "Preparing and loading the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zA3U816YksO_",
        "colab_type": "code",
        "outputId": "aa67cb05-4b10-4267-af72-a84d63ba8f06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "batch_size = 16\n",
        "\n",
        "train_lists = [X_train, y_train]\n",
        "test_lists = [X_test, y_test]\n",
        "\n",
        "training_dataset = text_dataset(x_y_list = train_lists )\n",
        "\n",
        "test_dataset = text_dataset(x_y_list = test_lists )\n",
        "\n",
        "dataloaders_dict = {'train': torch.utils.data.DataLoader(training_dataset, batch_size=batch_size, shuffle=True, num_workers=0),\n",
        "                   'val':torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "                   }\n",
        "dataset_sizes = {'train':len(train_lists[0]),\n",
        "                'val':len(test_lists[0])}\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVep1AogIndK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BertLayerNorm(nn.Module):\n",
        "        def __init__(self, hidden_size, eps=1e-12):\n",
        "            \"\"\"Construct a layernorm module in the TF style (epsilon inside the square root).\n",
        "            \"\"\"\n",
        "            super(BertLayerNorm, self).__init__()\n",
        "            self.weight = nn.Parameter(torch.ones(hidden_size))\n",
        "            self.bias = nn.Parameter(torch.zeros(hidden_size))\n",
        "            self.variance_epsilon = eps\n",
        "\n",
        "        def forward(self, x):\n",
        "            u = x.mean(-1, keepdim=True)\n",
        "            s = (x - u).pow(2).mean(-1, keepdim=True)\n",
        "            x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n",
        "            return self.weight * x + self.bias\n",
        "        \n",
        "\n",
        "class BertForSequenceClassification(nn.Module):\n",
        "    \"\"\"BERT model for classification.\n",
        "    This module is composed of the BERT model with a linear layer on top of\n",
        "    the pooled output.\n",
        "    Params:\n",
        "        `config`: a BertConfig class instance with the configuration to build a new model.\n",
        "        `num_labels`: the number of classes for the classifier. Default = 2.\n",
        "    Inputs:\n",
        "        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
        "            with the word token indices in the vocabulary. Items in the batch should begin with the special \"CLS\" token. (see the tokens preprocessing logic in the scripts\n",
        "            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
        "        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n",
        "            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n",
        "            a `sentence B` token (see BERT paper for more details).\n",
        "        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n",
        "            selected in [0, 1]. It's a mask to be used if the input sequence length is smaller\n",
        "            than the max\n",
        "            input sequence length in the current batch. It's the mask that we typically use for attention when\n",
        "            a batch has varying length sentences.\n",
        "        `labels`: labels for the classification output: torch.LongTensor of shape [batch_size]\n",
        "            with indices selected in [0, ..., num_labels].\n",
        "    Outputs:\n",
        "        if `labels` is not `None`:\n",
        "            Outputs the CrossEntropy classification loss of the output with the labels.\n",
        "        if `labels` is `None`:\n",
        "            Outputs the classification logits of shape [batch_size, num_labels].\n",
        "    Example usage:\n",
        "    ```python\n",
        "    # Already been converted into WordPiece token ids\n",
        "    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
        "    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
        "    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n",
        "    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n",
        "        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
        "    num_labels = 2\n",
        "    model = BertForSequenceClassification(config, num_labels)\n",
        "    logits = model(input_ids, token_type_ids, input_mask)\n",
        "    ```\n",
        "    \"\"\"\n",
        "    def __init__(self, num_labels=23):\n",
        "        super(BertForSequenceClassification, self).__init__()\n",
        "        self.num_labels = num_labels\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
        "        nn.init.xavier_normal_(self.classifier.weight)\n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
        "        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "        return logits\n",
        "    def freeze_bert_encoder(self):\n",
        "        for param in self.bert.parameters():\n",
        "            param.requires_grad = False\n",
        "    \n",
        "    def unfreeze_bert_encoder(self):\n",
        "        for param in self.bert.parameters():\n",
        "            param.requires_grad = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLBh7tlgI1it",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pytorch_pretrained_bert import BertConfig\n",
        "\n",
        "config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n",
        "        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
        "\n",
        "num_labels = 23\n",
        "model = BertForSequenceClassification(num_labels)\n",
        "\n",
        "# Convert inputs to PyTorch tensors\n",
        "#tokens_tensor = torch.tensor([tokenizer.convert_tokens_to_ids(zz)])\n",
        "\n",
        "#logits = model(tokens_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7B4rLMnFR4o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()\n",
        "    print('starting')\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_loss = 100\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            \n",
        "            sentiment_corrects = 0\n",
        "            \n",
        "            \n",
        "            # Iterate over data.\n",
        "            for inputs, sentiment in dataloaders_dict[phase]:\n",
        "                #print(\"ok till here\")\n",
        "                #inputs = inputs\n",
        "                #print(len(inputs),type(inputs),inputs)\n",
        "                #inputs = torch.from_numpy(np.array(inputs)).to(device) \n",
        "                inputs = inputs.to(device) \n",
        "\n",
        "                sentiment = sentiment.to(device)\n",
        "                \n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        " \n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    #print(inputs)\n",
        "                    outputs = model(inputs)\n",
        "\n",
        "                    outputs = F.softmax(outputs,dim = 1)\n",
        "                    \n",
        "                    loss = criterion(outputs, torch.max(sentiment.float(), 1)[1])\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        \n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "                \n",
        "                sentiment_corrects += torch.sum(torch.max(outputs, 1)[1] == torch.max(sentiment, 1)[1])\n",
        "\n",
        "                \n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "\n",
        "            \n",
        "            sentiment_acc = sentiment_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print('{} total loss: {:.4f} '.format(phase,epoch_loss ))\n",
        "            print('{} sentiment_acc: {:.4f}'.format(\n",
        "                phase, sentiment_acc))\n",
        "\n",
        "            if phase == 'val' and epoch_loss < best_loss:\n",
        "                print('saving with loss of {}'.format(epoch_loss),\n",
        "                      'improved over previous {}'.format(best_loss))\n",
        "                best_loss = epoch_loss\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                torch.save(model.state_dict(), 'bert_model_test.pth')\n",
        "\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(float(best_loss)))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDoE-BzSH41N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lrlast = .001\n",
        "lrmain = .00001\n",
        "optim1 = optim.Adam(\n",
        "    [\n",
        "        {\"params\":model.bert.parameters(),\"lr\": lrmain},\n",
        "        {\"params\":model.classifier.parameters(), \"lr\": lrlast},\n",
        "       \n",
        "   ])\n",
        "\n",
        "#optim1 = optim.Adam(model.parameters(), lr=0.001)#,momentum=.9)\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim1\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=3, gamma=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMKcj7f5J0KM",
        "colab_type": "code",
        "outputId": "b51df1c4-8c48-484b-b6c7-175c9ddc411e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.to(device)\n",
        "model_ft1 = train_model(model, criterion, optimizer_ft, exp_lr_scheduler,\n",
        "                       num_epochs=10)"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "starting\n",
            "Epoch 0/9\n",
            "----------\n",
            "train total loss: 2.2560 \n",
            "train sentiment_acc: 0.9515\n",
            "val total loss: 2.3220 \n",
            "val sentiment_acc: 0.8864\n",
            "saving with loss of 2.321974515914917 improved over previous 100\n",
            "\n",
            "Epoch 1/9\n",
            "----------\n",
            "train total loss: 2.2560 \n",
            "train sentiment_acc: 0.9515\n",
            "val total loss: 2.3220 \n",
            "val sentiment_acc: 0.8864\n",
            "saving with loss of 2.3219731504266914 improved over previous 2.321974515914917\n",
            "\n",
            "Epoch 2/9\n",
            "----------\n",
            "train total loss: 2.2560 \n",
            "train sentiment_acc: 0.9515\n",
            "val total loss: 2.3220 \n",
            "val sentiment_acc: 0.8864\n",
            "saving with loss of 2.3219725868918677 improved over previous 2.3219731504266914\n",
            "\n",
            "Epoch 3/9\n",
            "----------\n",
            "train total loss: 2.2560 \n",
            "train sentiment_acc: 0.9515\n",
            "val total loss: 2.3220 \n",
            "val sentiment_acc: 0.8864\n",
            "saving with loss of 2.321972543543035 improved over previous 2.3219725868918677\n",
            "\n",
            "Epoch 4/9\n",
            "----------\n",
            "train total loss: 2.2560 \n",
            "train sentiment_acc: 0.9515\n",
            "val total loss: 2.3220 \n",
            "val sentiment_acc: 0.8864\n",
            "saving with loss of 2.32197228345004 improved over previous 2.321972543543035\n",
            "\n",
            "Epoch 5/9\n",
            "----------\n",
            "train total loss: 2.2560 \n",
            "train sentiment_acc: 0.9515\n",
            "val total loss: 2.3220 \n",
            "val sentiment_acc: 0.8864\n",
            "\n",
            "Epoch 6/9\n",
            "----------\n",
            "train total loss: 2.2560 \n",
            "train sentiment_acc: 0.9515\n",
            "val total loss: 2.3220 \n",
            "val sentiment_acc: 0.8864\n",
            "\n",
            "Epoch 7/9\n",
            "----------\n",
            "train total loss: 2.2560 \n",
            "train sentiment_acc: 0.9515\n",
            "val total loss: 2.3220 \n",
            "val sentiment_acc: 0.8864\n",
            "\n",
            "Epoch 8/9\n",
            "----------\n",
            "train total loss: 2.2560 \n",
            "train sentiment_acc: 0.9515\n",
            "val total loss: 2.3220 \n",
            "val sentiment_acc: 0.8864\n",
            "saving with loss of 2.321972218426791 improved over previous 2.32197228345004\n",
            "\n",
            "Epoch 9/9\n",
            "----------\n",
            "train total loss: 2.2561 \n",
            "train sentiment_acc: 0.9515\n",
            "val total loss: 2.3220 \n",
            "val sentiment_acc: 0.8864\n",
            "saving with loss of 2.3219721750779585 improved over previous 2.321972218426791\n",
            "\n",
            "Training complete in 4m 29s\n",
            "Best val Acc: 2.321972\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67OKez2ecjDg",
        "colab_type": "text"
      },
      "source": [
        "# **TESTING**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sP_L6Hz_ybT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#final_test = pd.read_csv('test.tsv',delimiter='\\t',encoding='utf-8')\n",
        "#X_final_test = final_test['Phrase']\n",
        "#X_final_test = X_final_test.values.tolist()\n",
        "X_final_test = 'I am having distressing dreams'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7TJU0QQeYxL",
        "colab_type": "text"
      },
      "source": [
        "Define a function to preprocess the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvT8bf2OtU_U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_seq_length = 256\n",
        "reviews=[]\n",
        "def test_dataset(Dataset):\n",
        "    for i in range(0,len(Dataset)):\n",
        "      tokenized_review = tokenizer.tokenize(Dataset[i])\n",
        "\n",
        "      if len(tokenized_review) > max_seq_length:\n",
        "          tokenized_review = tokenized_review[:max_seq_length]\n",
        "\n",
        "      ids_review  = tokenizer.convert_tokens_to_ids(tokenized_review)\n",
        "\n",
        "      padding = [0] * (max_seq_length - len(ids_review))\n",
        "\n",
        "      ids_review += padding\n",
        "\n",
        "      assert len(ids_review) == max_seq_length\n",
        "\n",
        "      #print(\"Hello\")\n",
        "      reviews.append(ids_review)\n",
        "\n",
        "    #sentiment = self.x_y_list[1][index] # color        \n",
        "    #list_of_labels = [torch.from_numpy(np.array(sentiment))]\n",
        "\n",
        "\n",
        "    return reviews#, list_of_labels[0]\n",
        "\n",
        "#def __len__(self):\n",
        " #   return len(self.x_list)\n",
        "X_final_test = test_dataset(X_final_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mkc5BXD-eun9",
        "colab_type": "text"
      },
      "source": [
        "Define the model and the configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wn_REsL6c4Z0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pytorch_pretrained_bert import BertConfig\n",
        "\n",
        "config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n",
        "        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
        "\n",
        "num_labels = 23\n",
        "model = BertForSequenceClassification(num_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBCAQx5qe2Tn",
        "colab_type": "text"
      },
      "source": [
        "Load the previously saved model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfE191JybN1O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_state_dict(torch.load('bert_model_test.pth'))\n",
        "device = torch.device('cuda')\n",
        "model.to(device)\n",
        "for param in model.parameters():\n",
        "    param.requires_grad=False\n",
        "model.eval()\n",
        "test_preds = np.zeros((1,23))\n",
        "tests =  torch.utils.data.TensorDataset(torch.tensor(X_final_test))\n",
        "test_loader = torch.utils.data.DataLoader(tests, batch_size=1, shuffle=False)\n",
        "\n",
        "for i,(x_batch,)  in enumerate(test_loader):\n",
        "    t_preds = model(x_batch.to(device), attention_mask=(x_batch>0).to(device), labels=None)\n",
        "    test_preds[i:i+1]=t_preds[:,0:23].detach().cpu().squeeze().numpy()\n",
        "#test_preds = t_preds[:,0:23].detach().cpu().squeeze.numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUYt2GnCfBUH",
        "colab_type": "text"
      },
      "source": [
        "Run our test list through the model and write the results to a file called test_preds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxACSX5KhFb0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "e4599d29-4d29-458d-e1f9-3d37b227ea06"
      },
      "source": [
        "t_p = pd.DataFrame(test_preds)\n",
        "def softmax(x):\n",
        "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "test_preds1 = t_p.apply(lambda x:softmax(x), axis=1)\n",
        "test_preds1"
      ],
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.997256</td>\n",
              "      <td>0.000415</td>\n",
              "      <td>0.000092</td>\n",
              "      <td>0.000224</td>\n",
              "      <td>0.000078</td>\n",
              "      <td>0.00022</td>\n",
              "      <td>0.000102</td>\n",
              "      <td>0.000074</td>\n",
              "      <td>0.000097</td>\n",
              "      <td>0.000131</td>\n",
              "      <td>0.000054</td>\n",
              "      <td>0.000043</td>\n",
              "      <td>0.000113</td>\n",
              "      <td>0.000115</td>\n",
              "      <td>0.000057</td>\n",
              "      <td>0.000122</td>\n",
              "      <td>0.000064</td>\n",
              "      <td>0.000152</td>\n",
              "      <td>0.000068</td>\n",
              "      <td>0.000267</td>\n",
              "      <td>0.000044</td>\n",
              "      <td>0.000076</td>\n",
              "      <td>0.000137</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         0         1         2   ...        20        21        22\n",
              "0  0.997256  0.000415  0.000092  ...  0.000044  0.000076  0.000137\n",
              "\n",
              "[1 rows x 23 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 219
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YANaL0t0foAu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.DataFrame(test_preds1).to_csv(\"test_preds1.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQHzO45lUCdg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duviFKynUUTy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}